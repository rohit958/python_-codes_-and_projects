{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "962ff710-ee56-4d77-b88a-84b71c06c783",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "schema1= StructType([StructField(\"Order_ID\", IntegerType()),\n",
    "    StructField(\"Order_Date\",DateType()),\n",
    "    StructField(\"Order_value\",IntegerType())\n",
    "])\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "path=\"dbfs:/FileStore/file-1.csv\"\n",
    "df=spark.read.option(\"sep\",\"\\t\").option(\"header\",\"false\").option(\"mode\",\"permissive\").option(\"dateFormat\",\"dd-MM-YYYY\").csv(path,schema1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94838fdf-cf6e-49a6-a221-7f3c7cdc7d1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+\n|Order_ID|Order_Date|Order_value|\n+--------+----------+-----------+\n|       1|2024-08-22|        125|\n|       2|2024-08-22|        355|\n|       3|2024-08-23|       4642|\n|       4|2024-08-24|        355|\n|       5|2024-08-23|        377|\n|       6|2024-08-26|        244|\n|       7|2024-08-25|        599|\n|       8|2024-08-24|       1200|\n|       9|2024-08-25|        450|\n|      10|2024-08-22|        960|\n+--------+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62038673-40b8-43fd-92ee-272ec7ff35a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n|Order_Date|running_total|\n+----------+-------------+\n|2024-08-25|         1049|\n|2024-08-23|         6068|\n|2024-08-24|         7623|\n|2024-08-26|         7867|\n|2024-08-22|         9307|\n+----------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"temp_tbl\")\n",
    "\n",
    "sql=\"\"\" select *, sum(Order_value) over(rows between unbounded preceding and current row) as running_total from temp_tbl\n",
    "order by Order_date;\n",
    "\"\"\"\n",
    "\n",
    "sql2=\"\"\"with cte as(select Order_Date,sum(order_value)as total from temp_tbl group by Order_Date)\n",
    "    select Order_Date, sum(total) over(rows between unbounded preceding and current row) as running_total from cte\n",
    "    ;\n",
    "    \"\"\"\n",
    "spark.sql(sql2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd778414-ed06-4563-b069-f085381dee78",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------------+\n|Order_Date|Total|Running_Total|\n+----------+-----+-------------+\n|2024-08-22| 1440|         1440|\n|2024-08-23| 5019|         6459|\n|2024-08-24| 1555|         8014|\n|2024-08-25| 1049|         9063|\n|2024-08-26|  244|         9307|\n+----------+-----+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "windowSpec=Window.partitionBy().rowsBetween(Window.unboundedPreceding,Window.currentRow)\n",
    "\n",
    "df1=df.groupBy(col(\"Order_Date\")).agg(sum(col(\"Order_value\")).alias(\"Total\")).orderBy(col(\"Order_Date\"))\n",
    "df1.withColumn(\"Running_Total\",sum(\"Total\").over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b197940b-cc53-4f9b-946e-145c463ceb45",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------------+\n|Order_date|Total|Running_Total|\n+----------+-----+-------------+\n|2024-08-22| 1440|         1440|\n|2024-08-23| 5019|         6459|\n|2024-08-24| 1555|         8014|\n|2024-08-25| 1049|         9063|\n|2024-08-26|  244|         9307|\n+----------+-----+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "windowSpec=Window.partitionBy().orderBy(col(\"Order_Date\")).rowsBetween(Window.unboundedPreceding,0)\n",
    "windowTotal=Window.partitionBy(\"Order_Date\")\n",
    "\n",
    "df1=df.withColumn(\"Total\",sum(\"Order_value\").over(windowTotal))\n",
    "df2=df1.select(\"Order_date\",\"Total\").distinct()\n",
    "\n",
    "\n",
    "\n",
    "df3=df2.withColumn(\"Running_Total\",sum(\"Total\").over(windowSpec))\n",
    "df3.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "TCS",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
