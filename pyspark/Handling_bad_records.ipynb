{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c95dbaa-f1e1-455b-b334-8920774f85c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "306e2dac-9ac5-4c10-845e-ae416e51ab53",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Modes:\n",
    "1. premissive :- inserts all the data and places null value in place of bad record. By default pypsark considers this mode.\n",
    "2. falifast:- the code exits when the bad record is found.\n",
    "3. dropmalformed:- drops the entire row if any corrupted record is found. But the code runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df0c23dd-e8b9-41d3-8534-b4681f5acc76",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "permissive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "220f87ed-09f6-4ae7-9b8d-892da292d637",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "schema = StructType([\n",
    "    StructField(\"EmployeeID\", IntegerType(), True),\n",
    "    StructField(\"EmployeeName\", StringType(), True),\n",
    "    StructField(\"Address\", StringType(), True),\n",
    "    StructField(\"_corrupt_record\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f7acb56-00c0-4133-a596-8a9435269133",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "employee_df = spark.read.option(\"mode\", \"PERMISSIVE\")\n",
    "    .schema(schema)\n",
    "    .option(\"header\", True)\n",
    "    .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")\n",
    "    .csv(\"./employee.csv\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42dbb137-5206-4873-b48a-7e1af21481d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b24f59b-34bc-4bbb-8d22-ebab2addc5c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Handling_bad_records",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
