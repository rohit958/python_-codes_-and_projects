{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Direction: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Weekday: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Commodity: string (nullable = true)\n",
      " |-- Transport_Mode: string (nullable = true)\n",
      " |-- Measure: string (nullable = true)\n",
      " |-- Value: integer (nullable = true)\n",
      " |-- Cumulative: long (nullable = true)\n",
      "\n",
      "+---------+----+----------+--------+-------+---------+--------------+-------+---------+----------+\n",
      "|Direction|Year|      Date| Weekday|Country|Commodity|Transport_Mode|Measure|    Value|Cumulative|\n",
      "+---------+----+----------+--------+-------+---------+--------------+-------+---------+----------+\n",
      "|  Exports|2015|01/01/2015|Thursday|    All|      All|           All|      $|104000000| 104000000|\n",
      "|  Exports|2015|02/01/2015|  Friday|    All|      All|           All|      $| 96000000| 200000000|\n",
      "|  Exports|2015|03/01/2015|Saturday|    All|      All|           All|      $| 61000000| 262000000|\n",
      "+---------+----+----------+--------+-------+---------+--------------+-------+---------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfc=spark.read.option('header',True).option('Inferschema',True).csv(\"D:\\git\\python_-codes_-and_projects\\sample_CSV\\covid.csv\")\n",
    "dfc.printSchema()\n",
    "dfc.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+--------+-------+---------+--------------+-------+---------+----------+\n",
      "|Direction|Year|      Date| Weekday|Country|Commodity|Transport_Mode|Measure|    Value|Cumulative|\n",
      "+---------+----+----------+--------+-------+---------+--------------+-------+---------+----------+\n",
      "|  Exports|2015|01/01/2015|Thursday|    All|      All|           All|      $|104000000| 104000000|\n",
      "|  Exports|2015|02/01/2015|  Friday|    All|      All|           All|      $| 96000000| 200000000|\n",
      "|  Exports|2015|03/01/2015|Saturday|    All|      All|           All|      $| 61000000| 262000000|\n",
      "|  Exports|2015|04/01/2015|  Sunday|    All|      All|           All|      $| 74000000| 336000000|\n",
      "|  Exports|2015|05/01/2015|  Monday|    All|      All|           All|      $|105000000| 442000000|\n",
      "+---------+----+----------+--------+-------+---------+--------------+-------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dataframe select and login operations\n",
    "dfc.select(\"Date\",\"Year\")\n",
    "dfc.select(dfc.Date,dfc.Year)\n",
    "\n",
    "\n",
    "dfc.select(\"*\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|      Date| Weekday|\n",
      "+----------+--------+\n",
      "|01/01/2015|Thursday|\n",
      "|02/01/2015|  Friday|\n",
      "|03/01/2015|Saturday|\n",
      "|04/01/2015|  Sunday|\n",
      "+----------+--------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select using indexes\n",
    "from pyspark.sql.functions import col\n",
    "dfc.select(dfc.columns[2:4]).show(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Direction: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Weekday: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Commodity: string (nullable = true)\n",
      " |-- Transport_Mode: string (nullable = true)\n",
      " |-- Measure: string (nullable = true)\n",
      " |-- Value: integer (nullable = true)\n",
      " |-- Cumulative: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.option('header',True).option('Inferschema',True).csv(\"D:\\git\\python_-codes_-and_projects\\sample_CSV\\covid.csv\")\n",
    "df.printSchema()\n",
    "df.createOrReplaceTempView(\"table1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+---------+-------+---------+--------------+-------+---------+----------+\n",
      "|Direction|Year|      Date|  Weekday|Country|Commodity|Transport_Mode|Measure|    Value|Cumulative|\n",
      "+---------+----+----------+---------+-------+---------+--------------+-------+---------+----------+\n",
      "|  Exports|2015|01/01/2015| Thursday|    All|      All|           All|      $|104000000| 104000000|\n",
      "|  Exports|2015|02/01/2015|   Friday|    All|      All|           All|      $| 96000000| 200000000|\n",
      "|  Exports|2015|03/01/2015| Saturday|    All|      All|           All|      $| 61000000| 262000000|\n",
      "|  Exports|2015|04/01/2015|   Sunday|    All|      All|           All|      $| 74000000| 336000000|\n",
      "|  Exports|2015|05/01/2015|   Monday|    All|      All|           All|      $|105000000| 442000000|\n",
      "|  Exports|2015|06/01/2015|  Tuesday|    All|      All|           All|      $| 76000000| 518000000|\n",
      "|  Exports|2015|07/01/2015|Wednesday|    All|      All|           All|      $| 59000000| 577000000|\n",
      "|  Exports|2015|08/01/2015| Thursday|    All|      All|           All|      $|111000000| 688000000|\n",
      "|  Exports|2015|09/01/2015|   Friday|    All|      All|           All|      $| 98000000| 786000000|\n",
      "|  Exports|2015|10/01/2015| Saturday|    All|      All|           All|      $| 89000000| 875000000|\n",
      "|  Exports|2015|11/01/2015|   Sunday|    All|      All|           All|      $|111000000| 986000000|\n",
      "|  Exports|2015|12/01/2015|   Monday|    All|      All|           All|      $|161000000|1147000000|\n",
      "|  Exports|2015|13/01/2015|  Tuesday|    All|      All|           All|      $| 96000000|1242000000|\n",
      "|  Exports|2015|14/01/2015|Wednesday|    All|      All|           All|      $| 83000000|1325000000|\n",
      "|  Exports|2015|15/01/2015| Thursday|    All|      All|           All|      $|109000000|1434000000|\n",
      "|  Exports|2015|16/01/2015|   Friday|    All|      All|           All|      $|155000000|1589000000|\n",
      "|  Exports|2015|17/01/2015| Saturday|    All|      All|           All|      $|168000000|1757000000|\n",
      "|  Exports|2015|18/01/2015|   Sunday|    All|      All|           All|      $| 90000000|1847000000|\n",
      "|  Exports|2015|19/01/2015|   Monday|    All|      All|           All|      $|164000000|2011000000|\n",
      "|  Exports|2015|20/01/2015|  Tuesday|    All|      All|           All|      $|134000000|2145000000|\n",
      "+---------+----+----------+---------+-------+---------+--------------+-------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query='''Select * from table1'''\n",
    "\n",
    "spark.sql(query).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "835930cdab659cfe0998f909845533c8bce739076c1411aa039e952001a7acf8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
