{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b932dfd-3a38-4799-80c0-1ce3dec5c35f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df_data = [\n",
    "    ('A', 'Good', 9.5, 'horror'),\n",
    "    ('B', 'Good', 8.2, 'Comedy'),\n",
    "    ('C', 'Avg', 9.5, 'Comedy'),\n",
    "    ('D', 'Avg', 9.5, 'horror')\n",
    "]\n",
    "df_columns = [\"title\", \"reviews\", \"ratings\", \"categories\"]\n",
    "df = spark.createDataFrame(df_data, df_columns)\n",
    "\n",
    "casting_df_data = [\n",
    "    ('Actor1', 'A', 9.0),\n",
    "    ('Actor2', 'B', 8.5),\n",
    "    ('Actor3', 'C', 7.5),\n",
    "    ('Actor4', 'D', 8.0)\n",
    "]\n",
    "casting_df_columns = [\"actor_name\", \"movie_name\", \"actor_rating\"]\n",
    "\n",
    "casting_df = spark.createDataFrame(casting_df_data, casting_df_columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e64f21e-9aa8-451d-97c8-0d32a2776d64",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "questions\n",
    "Q1) join df and casting_df on movie name.\n",
    "Q2) Find the category of the actor and movie.\n",
    "Q3) Find the what is the avg rating of actor\n",
    "Q4) how to sort in pyspark?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a6d346b-0152-4a87-8eca-83dc23d7edc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------+----------+----------+----------+------------+\n|title|reviews|ratings|categories|actor_name|movie_name|actor_rating|\n+-----+-------+-------+----------+----------+----------+------------+\n|    A|   Good|    9.5|    horror|    Actor1|         A|         9.0|\n|    B|   Good|    8.2|    Comedy|    Actor2|         B|         8.5|\n|    C|    Avg|    9.5|    Comedy|    Actor3|         C|         7.5|\n|    D|    Avg|    9.5|    horror|    Actor4|         D|         8.0|\n+-----+-------+-------+----------+----------+----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "#q1\n",
    "\n",
    "joined_df=df.join(casting_df,df.title==casting_df.movie_name,\"inner\")\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77a17224-f5c9-4dfd-b6f9-e23e41979663",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----------+\n|title|categories|actor_name|\n+-----+----------+----------+\n|    A|    horror|    Actor1|\n|    B|    Comedy|    Actor2|\n|    C|    Comedy|    Actor3|\n|    D|    horror|    Actor4|\n+-----+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#q2\n",
    "\n",
    "joined_df.select(\"title\",\"categories\",\"actor_name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c947fb9-214d-4085-9ee3-9099e1941543",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fb5b54a-2399-4973-acaf-e3732a264446",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n|avg(actor_rating)|\n+-----------------+\n|             8.25|\n+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "#q3\n",
    "joined_df.agg(avg(\"actor_rating\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59eedaab-8d13-462b-a4e7-f4ec0c6781d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------+----------+----------+----------+------------+\n|title|reviews|ratings|categories|actor_name|movie_name|actor_rating|\n+-----+-------+-------+----------+----------+----------+------------+\n|    A|   Good|    9.5|    horror|    Actor1|         A|         9.0|\n|    C|    Avg|    9.5|    Comedy|    Actor3|         C|         7.5|\n|    D|    Avg|    9.5|    horror|    Actor4|         D|         8.0|\n|    B|   Good|    8.2|    Comedy|    Actor2|         B|         8.5|\n+-----+-------+-------+----------+----------+----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "#q4\n",
    "joined_df.orderBy(\"ratings\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef6d481c-d297-4b9c-9e71-5159ca3bf190",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+\n| ID| Name|Boss|\n+---+-----+----+\n|  1|Alice|NULL|\n|  2|  Bob|   1|\n|  3|Carol|   2|\n|  4| Dave|   1|\n|  5|  Eve|   2|\n|  6|Frank|   4|\n+---+-----+----+\n\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, \"Alice\", None),\n",
    "    (2, \"Bob\", 1),\n",
    "    (3, \"Carol\", 2),\n",
    "    (4, \"Dave\", 1),\n",
    "    (5, \"Eve\", 2),\n",
    "    (6, \"Frank\", 4)\n",
    "]\n",
    "\n",
    "schema = [\"ID\", \"Name\", \"Boss\"]\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f158ac43-5082-4920-a667-47839b7e75b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---------+\n| ID| Name|Boss_name|\n+---+-----+---------+\n|  1|Alice|  No Boss|\n|  2|  Bob|    Alice|\n|  3|Carol|      Bob|\n|  4| Dave|    Alice|\n|  5|  Eve|      Bob|\n|  6|Frank|     Dave|\n+---+-----+---------+\n\n"
     ]
    }
   ],
   "source": [
    "#Self Join\n",
    "df_joined=df.alias(\"df1\").join(df.alias(\"df2\"), col(\"df1.Boss\") == col(\"df2.ID\"),\"left\")\\\n",
    "    .select(col(\"df1.ID\"),col(\"df1.Name\"), col(\"df2.Name\").alias(\"Boss_name\"))\n",
    "\n",
    "#Fill null values with \"No Boss\"\n",
    "df_joined=df_joined.fillna(\"No Boss\",subset=[\"Boss_name\"])\n",
    "df_joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26c8f811-9293-4e99-8a43-ebd642e0bca9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------------+\n|Empid|  Name|   Locations|\n+-----+------+------------+\n|    1|Gaurav|Pune,hyd,Blr|\n|    2|  Ravi|     hyd,Blr|\n+-----+------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "data=[(1,\"Gaurav\",\"Pune,hyd,Blr\"),(2,\"Ravi\",\"hyd,Blr\")]\n",
    "\n",
    "columns=[\"Empid\",\"Name\",\"Locations\"]\n",
    "df_emp=spark.createDataFrame(data,columns)\n",
    "\n",
    "df_emp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5624218e-9785-4d71-9fb5-881e35944fd3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Non Bengalore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1d7addc-9804-4bca-ba70-63094e7923db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+--------+\n|Empid|  Name|Location|\n+-----+------+--------+\n|    1|Gaurav|    Pune|\n|    1|Gaurav|     hyd|\n|    2|  Ravi|     hyd|\n+-----+------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df1=df_emp.select(\"Empid\",\"Name\", explode(split(col(\"Locations\"),\",\")))\\\n",
    "  .withColumnRenamed(\"col\",\"Location\").filter(col(\"Location\")!=\"Blr\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a927474-91e5-49e6-822b-b09ab156d2d9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "count the number of movies in each genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "290fcc3a-f72c-4089-bc7f-e90723595e44",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n|  genres|count(name)|\n+--------+-----------+\n|   Crime|          4|\n|   Drama|          4|\n|Thriller|          2|\n|  Action|          1|\n+--------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = spark.createDataFrame([('The Shawshank Redemption',['Drama', 'Crime']),\n",
    "                  ('The Godfather', ['Drama', 'Crime']),\n",
    "                  ('Pulp Fiction', ['Drama', 'Crime','Thriller']),\n",
    "                  ('The Dark Knight', ['Drama', 'Crime','Thriller','Action']),\n",
    "                  ],[\"name\", \"genres\"])\n",
    "\n",
    "df1=df.select(\"name\",explode(col(\"genres\")).alias(\"genres\"))\n",
    "\n",
    "df1.groupBy(\"genres\").agg(count(\"name\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c4b5145-671a-42c4-973f-f1560fd878d0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Question: Calculate the month-wise cumulative revenue using PySpark?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0853c2c2-6995-43f1-83da-ebb5c6895a8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark=SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\" )\n",
    "data = [( \n",
    "'3000' ,  '22-may'), \n",
    "('5000' ,  '23-may'),\n",
    "('5000' ,  '25-may'),\n",
    "('10000' , '22-june'),  \n",
    "('1250'  , '03-july')]\n",
    "\n",
    "schema = ['revenue','date']\n",
    "df_rev=spark.createDataFrame(data,schema)\n",
    "\n",
    "\n",
    "spark=SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11a98034-2e6f-4ac8-b7be-c7b02b0946bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+--------------+\n|revenue|   date|month|cumulative_sum|\n+-------+-------+-----+--------------+\n|   1250|03-JULY|  Jul|        1250.0|\n|  10000|22-JUNE|  Jun|       10000.0|\n|   3000| 22-MAY|  May|        3000.0|\n|   5000| 23-MAY|  May|        8000.0|\n|   5000| 25-MAY|  May|       13000.0|\n+-------+-------+-----+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "df=df_rev.withColumn(\"month\",date_format(to_date(col(\"date\"),\"dd-MMM\"),\"MMM\"))\n",
    "\n",
    "window_spec=Window.partitionBy('month').orderBy('date').rowsBetween(Window.unboundedPreceding,0)\n",
    "\n",
    "df=df.withColumn('cumulative_sum',sum(col(\"revenue\")).over(window_spec))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56fd10c4-32dd-41b3-b79a-979464b56ea9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>revenue</th><th>date</th><th>month</th><th>run_revenue</th></tr></thead><tbody><tr><td>1250</td><td>03-JULY</td><td>Jul</td><td>1250.0</td></tr><tr><td>10000</td><td>22-JUNE</td><td>Jun</td><td>10000.0</td></tr><tr><td>3000</td><td>22-MAY</td><td>May</td><td>3000.0</td></tr><tr><td>5000</td><td>23-MAY</td><td>May</td><td>8000.0</td></tr><tr><td>5000</td><td>25-MAY</td><td>May</td><td>13000.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "1250",
         "03-JULY",
         "Jul",
         1250.0
        ],
        [
         "10000",
         "22-JUNE",
         "Jun",
         10000.0
        ],
        [
         "3000",
         "22-MAY",
         "May",
         3000.0
        ],
        [
         "5000",
         "23-MAY",
         "May",
         8000.0
        ],
        [
         "5000",
         "25-MAY",
         "May",
         13000.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "revenue",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "month",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "run_revenue",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, date_format, to_date, sum, window, initcap\n",
    "\n",
    "windowSpec = Window.partitionBy(\"month\").orderBy(\"month\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "df_rev = df_rev.withColumn(\"date\", upper(col(\"date\")))  # Capitalize the first letter\n",
    "df_rev = df_rev.withColumn(\"month\", date_format(to_date(col(\"date\"), \"dd-MMM\"), \"MMM\"))\n",
    "df_final = df_rev.withColumn(\"run_revenue\", sum(\"revenue\").over(windowSpec))\n",
    "display(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0331afd3-1256-48a0-9f71-d91c2f9bc745",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+----------+\n|employee_id|first_name|last_name|manager_id|\n+-----------+----------+---------+----------+\n|       4529|     Nancy|    Young|      4125|\n|       4238|      John|    Simon|      4329|\n|       4329|   Martina| Candreva|      4125|\n|       4009|     Klaus|     Koch|      4329|\n|       4125|   Mafalda|  Ranieri|      NULL|\n|       4500|     Jakub|   Hrabal|      4529|\n|       4118|     Moira|    Areas|      4952|\n|       4012|       Jon|  Nilssen|      4952|\n|       4952|    Sandra| Rajkovic|      4529|\n|       4444|    Seamus|    Quinn|      4329|\n+-----------+----------+---------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [('4529', 'Nancy', 'Young', '4125'),\n",
    "('4238','John', 'Simon', '4329'),\n",
    "('4329', 'Martina', 'Candreva', '4125'),\n",
    "('4009', 'Klaus', 'Koch', '4329'),\n",
    "('4125', 'Mafalda', 'Ranieri', 'NULL'),\n",
    "('4500', 'Jakub', 'Hrabal', '4529'),\n",
    "('4118', 'Moira', 'Areas', '4952'),\n",
    "('4012', 'Jon', 'NULL', '4952'),\n",
    "('4952', 'Sandra', 'Rajkovic', '4529'),\n",
    "('4444', 'Seamus', 'Quinn', '4329')]\n",
    "\n",
    "schema = ['employee_id' ,'first_name', 'last_name', 'manager_id']\n",
    "\n",
    "df=spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1ed07b5-1eaf-4339-89da-c5bb2f3d1def",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n|manager|count|\n+-------+-----+\n|  Nancy|    2|\n|Martina|    3|\n|Mafalda|    2|\n| Sandra|    2|\n+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df_new=df.alias(\"e\").join(df.alias(\"m\"), col(\"e.manager_id\") == col(\"m.employee_id\"),\"inner\").select(col(\"e.employee_id\"),col(\"e.first_name\"), col(\"m.first_name\").alias(\"manager\"))\n",
    "\n",
    "df_new.groupBy(\"manager\").count().alias(\"no_of_emp\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99b27912-dabe-4664-82aa-cf587d8cecb6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+\n|manager_id|no_of_emp|mangr_name|\n+----------+---------+----------+\n|      4529|        2|     Nancy|\n|      4329|        3|   Martina|\n|      4125|        2|   Mafalda|\n|      4952|        2|    Sandra|\n+----------+---------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"emp\")\n",
    "query = '''select  e.manager_id as manager_id, \n",
    "count(e.employee_id) as no_of_emp,(m.First_name) as mangr_name \n",
    "from  emp e\n",
    "inner join emp m \n",
    "on m.employee_id =e.manager_id\n",
    "group by 1,3\n",
    "'''\n",
    "result=spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "952706d0-96a0-4977-9675-51a45f6b7c62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- employee_id: integer (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- manager_id: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "df=df.withColumn(\"employee_id\",col(\"employee_id\").cast(\"int\"))\\\n",
    "    .withColumn(\"manager_id\",col(\"manager_id\").cast(\"int\"))\n",
    "df.printSchema()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4a2e13c-311f-43c2-bcb3-fbf46e8012ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNumberFormatException\u001B[0m                     Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2436434663360693>, line 5\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m col\n",
       "\u001B[1;32m      3\u001B[0m df1 \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mselect([count(col(i)) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m df\u001B[38;5;241m.\u001B[39mcolumns])\n",
       "\u001B[0;32m----> 5\u001B[0m df1\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pyspark/sql/connect/dataframe.py:1159\u001B[0m, in \u001B[0;36mDataFrame.show\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n",
       "\u001B[1;32m   1158\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mshow\u001B[39m(\u001B[38;5;28mself\u001B[39m, n: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m20\u001B[39m, truncate: Union[\u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mint\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m, vertical: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[0;32m-> 1159\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_show_string\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtruncate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvertical\u001B[49m\u001B[43m)\u001B[49m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pyspark/sql/connect/dataframe.py:903\u001B[0m, in \u001B[0;36mDataFrame._show_string\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n",
       "\u001B[1;32m    886\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n",
       "\u001B[1;32m    887\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n",
       "\u001B[1;32m    888\u001B[0m             error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_BOOL\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    889\u001B[0m             message_parameters\u001B[38;5;241m=\u001B[39m{\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    892\u001B[0m             },\n",
       "\u001B[1;32m    893\u001B[0m         )\n",
       "\u001B[1;32m    895\u001B[0m table, _ \u001B[38;5;241m=\u001B[39m \u001B[43mDataFrame\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m    896\u001B[0m \u001B[43m    \u001B[49m\u001B[43mplan\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mShowString\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m    897\u001B[0m \u001B[43m        \u001B[49m\u001B[43mchild\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_plan\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    898\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_rows\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    899\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtruncate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_truncate\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    900\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvertical\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvertical\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    901\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    902\u001B[0m \u001B[43m    \u001B[49m\u001B[43msession\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_session\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[0;32m--> 903\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_to_table\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    904\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m table[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mas_py()\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pyspark/sql/connect/dataframe.py:1866\u001B[0m, in \u001B[0;36mDataFrame._to_table\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m   1864\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_to_table\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpa.Table\u001B[39m\u001B[38;5;124m\"\u001B[39m, Optional[StructType]]:\n",
       "\u001B[1;32m   1865\u001B[0m     query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mto_proto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n",
       "\u001B[0;32m-> 1866\u001B[0m     table, schema \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_session\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_table\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_plan\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mobservations\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1867\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m table \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   1868\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (table, schema)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pyspark/sql/connect/client/core.py:992\u001B[0m, in \u001B[0;36mSparkConnectClient.to_table\u001B[0;34m(self, plan, observations)\u001B[0m\n",
       "\u001B[1;32m    990\u001B[0m req \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_plan_request_with_metadata()\n",
       "\u001B[1;32m    991\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mCopyFrom(plan)\n",
       "\u001B[0;32m--> 992\u001B[0m table, schema, _, _, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execute_and_fetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mobservations\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    993\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m table \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    994\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m table, schema\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pyspark/sql/connect/client/core.py:1624\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n",
       "\u001B[1;32m   1621\u001B[0m schema: Optional[StructType] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   1622\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n",
       "\u001B[0;32m-> 1624\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n",
       "\u001B[1;32m   1625\u001B[0m     req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m []\n",
       "\u001B[1;32m   1626\u001B[0m ):\n",
       "\u001B[1;32m   1627\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n",
       "\u001B[1;32m   1628\u001B[0m         schema \u001B[38;5;241m=\u001B[39m response\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pyspark/sql/connect/client/core.py:1601\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata)\u001B[0m\n",
       "\u001B[1;32m   1599\u001B[0m                     \u001B[38;5;28;01myield from\u001B[39;00m handle_response(b)\n",
       "\u001B[1;32m   1600\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 1601\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43merror\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pyspark/sql/connect/client/core.py:1910\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   1908\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   1909\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n",
       "\u001B[0;32m-> 1910\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle_rpc_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43merror\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1911\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, \u001B[38;5;167;01mValueError\u001B[39;00m):\n",
       "\u001B[1;32m   1912\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pyspark/sql/connect/client/core.py:1985\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n",
       "\u001B[1;32m   1982\u001B[0m             info \u001B[38;5;241m=\u001B[39m error_details_pb2\u001B[38;5;241m.\u001B[39mErrorInfo()\n",
       "\u001B[1;32m   1983\u001B[0m             d\u001B[38;5;241m.\u001B[39mUnpack(info)\n",
       "\u001B[0;32m-> 1985\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n",
       "\u001B[1;32m   1986\u001B[0m                 info,\n",
       "\u001B[1;32m   1987\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   1988\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n",
       "\u001B[1;32m   1989\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n",
       "\u001B[1;32m   1990\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m   1992\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(status\u001B[38;5;241m.\u001B[39mmessage) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m   1993\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mNumberFormatException\u001B[0m: [CAST_INVALID_INPUT] The value 'NULL' of the type \"STRING\" cannot be cast to \"INT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error. SQLSTATE: 22018"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "NumberFormatException",
        "evalue": "[CAST_INVALID_INPUT] The value 'NULL' of the type \"STRING\" cannot be cast to \"INT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error. SQLSTATE: 22018"
       },
       "metadata": {
        "errorSummary": "[CAST_INVALID_INPUT] The value 'NULL' of the type \"STRING\" cannot be cast to \"INT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "CAST_INVALID_INPUT",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "sqlState": "22018",
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mNumberFormatException\u001B[0m                     Traceback (most recent call last)",
        "File \u001B[0;32m<command-2436434663360693>, line 5\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m col\n\u001B[1;32m      3\u001B[0m df1 \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mselect([count(col(i)) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m df\u001B[38;5;241m.\u001B[39mcolumns])\n\u001B[0;32m----> 5\u001B[0m df1\u001B[38;5;241m.\u001B[39mshow()\n",
        "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pyspark/sql/connect/dataframe.py:1159\u001B[0m, in \u001B[0;36mDataFrame.show\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n\u001B[1;32m   1158\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mshow\u001B[39m(\u001B[38;5;28mself\u001B[39m, n: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m20\u001B[39m, truncate: Union[\u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mint\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m, vertical: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1159\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_show_string\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtruncate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvertical\u001B[49m\u001B[43m)\u001B[49m)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pyspark/sql/connect/dataframe.py:903\u001B[0m, in \u001B[0;36mDataFrame._show_string\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n\u001B[1;32m    886\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n\u001B[1;32m    887\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[1;32m    888\u001B[0m             error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_BOOL\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    889\u001B[0m             message_parameters\u001B[38;5;241m=\u001B[39m{\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    892\u001B[0m             },\n\u001B[1;32m    893\u001B[0m         )\n\u001B[1;32m    895\u001B[0m table, _ \u001B[38;5;241m=\u001B[39m \u001B[43mDataFrame\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    896\u001B[0m \u001B[43m    \u001B[49m\u001B[43mplan\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mShowString\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    897\u001B[0m \u001B[43m        \u001B[49m\u001B[43mchild\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_plan\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    898\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_rows\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    899\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtruncate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_truncate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    900\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvertical\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvertical\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    901\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    902\u001B[0m \u001B[43m    \u001B[49m\u001B[43msession\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_session\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m--> 903\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_to_table\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    904\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m table[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mas_py()\n",
        "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pyspark/sql/connect/dataframe.py:1866\u001B[0m, in \u001B[0;36mDataFrame._to_table\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1864\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_to_table\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpa.Table\u001B[39m\u001B[38;5;124m\"\u001B[39m, Optional[StructType]]:\n\u001B[1;32m   1865\u001B[0m     query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mto_proto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n\u001B[0;32m-> 1866\u001B[0m     table, schema \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_session\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_table\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_plan\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mobservations\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1867\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m table \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1868\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (table, schema)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pyspark/sql/connect/client/core.py:992\u001B[0m, in \u001B[0;36mSparkConnectClient.to_table\u001B[0;34m(self, plan, observations)\u001B[0m\n\u001B[1;32m    990\u001B[0m req \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_plan_request_with_metadata()\n\u001B[1;32m    991\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mCopyFrom(plan)\n\u001B[0;32m--> 992\u001B[0m table, schema, _, _, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execute_and_fetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mobservations\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    993\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m table \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    994\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m table, schema\n",
        "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pyspark/sql/connect/client/core.py:1624\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n\u001B[1;32m   1621\u001B[0m schema: Optional[StructType] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1622\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m-> 1624\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n\u001B[1;32m   1625\u001B[0m     req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m []\n\u001B[1;32m   1626\u001B[0m ):\n\u001B[1;32m   1627\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n\u001B[1;32m   1628\u001B[0m         schema \u001B[38;5;241m=\u001B[39m response\n",
        "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pyspark/sql/connect/client/core.py:1601\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata)\u001B[0m\n\u001B[1;32m   1599\u001B[0m                     \u001B[38;5;28;01myield from\u001B[39;00m handle_response(b)\n\u001B[1;32m   1600\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 1601\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43merror\u001B[49m\u001B[43m)\u001B[49m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pyspark/sql/connect/client/core.py:1910\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   1908\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   1909\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 1910\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle_rpc_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43merror\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1911\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, \u001B[38;5;167;01mValueError\u001B[39;00m):\n\u001B[1;32m   1912\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
        "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pyspark/sql/connect/client/core.py:1985\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   1982\u001B[0m             info \u001B[38;5;241m=\u001B[39m error_details_pb2\u001B[38;5;241m.\u001B[39mErrorInfo()\n\u001B[1;32m   1983\u001B[0m             d\u001B[38;5;241m.\u001B[39mUnpack(info)\n\u001B[0;32m-> 1985\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   1986\u001B[0m                 info,\n\u001B[1;32m   1987\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   1988\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   1989\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   1990\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m   1992\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(status\u001B[38;5;241m.\u001B[39mmessage) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m   1993\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "\u001B[0;31mNumberFormatException\u001B[0m: [CAST_INVALID_INPUT] The value 'NULL' of the type \"STRING\" cannot be cast to \"INT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error. SQLSTATE: 22018"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df1 = df.select([count(col(i)) for i in df.columns])\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44668c18-b644-4228-b258-187a19877f06",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "from pyspark.sql.functions import *\n",
    "data = [1, 2, 3, 4, 6, 8, 10]\n",
    "schema = StructType([StructField(\"id\", IntegerType(), True)])\n",
    "\n",
    "df=spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9929d97-7bae-44cf-9b8e-319f217621b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n| id|\n+---+\n|  1|\n|  2|\n|  3|\n|  4|\n|  5|\n|  6|\n|  7|\n|  8|\n|  9|\n| 10|\n+---+\n\n"
     ]
    }
   ],
   "source": [
    "#Finding missing ID.\n",
    "minval=df.select(min(col('id'))).collect()[0][0]\n",
    "maxval=df.select(max(col('id'))).collect()[0][0]\n",
    "df1=spark.range(minval,maxdf+1)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bde250c5-73ec-4551-a3f2-f6e86952c5e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n| id|\n+---+\n|  5|\n|  7|\n|  9|\n+---+\n\n"
     ]
    }
   ],
   "source": [
    "df_missing=df1.join(df, df1.id==df.id, how='left_anti')\n",
    "df_missing.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pyspark_Delloite",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
